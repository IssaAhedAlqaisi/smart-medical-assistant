{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiE3KQwmKHQz",
        "outputId": "1e9e1921-ccb8-4f85-ef3d-3eb872b3a812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q transformers sentence-transformers pymupdf langchain faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import fitz\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load models\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "# Global state\n",
        "chunks = []\n",
        "index = None\n",
        "original_chunks = []\n",
        "chat_history = \"\"  # Ù„Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©\n",
        "\n",
        "# Utils\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=300):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def build_faiss_index(chunks):\n",
        "    embeddings = [embedder.encode(chunk) for chunk in chunks]\n",
        "    dim = len(embeddings[0])\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(np.array(embeddings))\n",
        "    return index, embeddings\n",
        "\n",
        "def load_pdf(file_path):\n",
        "    global chunks, index, original_chunks, chat_history\n",
        "    text = extract_text_from_pdf(file_path)\n",
        "    chunks = chunk_text(text)\n",
        "    index, _ = build_faiss_index(chunks)\n",
        "    original_chunks = chunks\n",
        "    chat_history = \"\"  # reset chat\n",
        "    return \"âœ… Medical PDF loaded successfully!\", \"\"\n",
        "\n",
        "def chat_with_pdf(user_input):\n",
        "    global chat_history\n",
        "    if not index:\n",
        "        return chat_history + \"\\nâŒ Please upload a PDF first.\"\n",
        "\n",
        "    q_emb = embedder.encode(user_input)\n",
        "    D, I = index.search(np.array([q_emb]), k=3)\n",
        "    top_chunks = [original_chunks[i] for i in I[0]]\n",
        "    context = \" \".join(top_chunks)\n",
        "    result = qa_pipeline(question=user_input, context=context)\n",
        "    answer = result['answer']\n",
        "\n",
        "    # ğŸ‘¤ = Ø´Ø®Øµ Ø¹Ø§Ø¯ÙŠ\n",
        "    chat_history += f\"\\nğŸ‘¤ You: {user_input}\\nğŸ¤– Assistant: {answer}\\n\\n\"\n",
        "    return chat_history\n",
        "\n",
        "# Build UI\n",
        "with gr.Blocks(css=\".gr-button {background-color: #2563eb !important; color: white} .gr-markdown {font-size: 16px;}\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <div style=\"text-align:center; padding: 20px;\">\n",
        "        <h1 style=\"color:#1e3a8a;\"> Smart Medical Assistant</h1>\n",
        "        <p style=\"font-size:18px;\">Upload a medical PDF and chat with it.</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_input = gr.File(type=\"filepath\", label=\"ğŸ“„ Upload Medical PDF\", file_types=[\".pdf\"])\n",
        "        status = gr.Textbox(label=\"ğŸŸ¢ Status\", interactive=False)\n",
        "\n",
        "    chatbox = gr.Textbox(label=\"ğŸ§¾ Chat\", lines=20, interactive=False, placeholder=\"Your conversation will appear here...\")\n",
        "    user_input = gr.Textbox(label=\"âœï¸ Type your question and press Enter\", placeholder=\"e.g. What is the diagnosis?\", lines=1)\n",
        "\n",
        "    pdf_input.change(fn=load_pdf, inputs=pdf_input, outputs=[status, chatbox])\n",
        "    user_input.submit(fn=chat_with_pdf, inputs=user_input, outputs=chatbox)\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "5kctz8SaKfk9",
        "outputId": "df997620-6b63-4def-8cb2-041da6b62c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://29c82aaacb1e08b95b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://29c82aaacb1e08b95b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}