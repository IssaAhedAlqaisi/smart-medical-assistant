# -*- coding: utf-8 -*-
"""Medicl_RAG_Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GQ8rleDZowb4GSfGhT7ST2JnaY7BMDj8
"""

# Step 1: Install dependencies
!pip install -q transformers sentence-transformers pymupdf langchain faiss-cpu

import gradio as gr
import fitz
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Load models
embedder = SentenceTransformer("all-MiniLM-L6-v2")
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Global state
chunks = []
index = None
original_chunks = []
chat_history = ""  # To store the conversation

# Utils
def extract_text_from_pdf(pdf_path):
    text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            text += page.get_text()
    return text

def chunk_text(text, chunk_size=300):
    words = text.split()
    return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

def build_faiss_index(chunks):
    embeddings = [embedder.encode(chunk) for chunk in chunks]
    dim = len(embeddings[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings))
    return index, embeddings

def load_pdf(file_path):
    global chunks, index, original_chunks, chat_history
    text = extract_text_from_pdf(file_path)
    chunks = chunk_text(text)
    index, _ = build_faiss_index(chunks)
    original_chunks = chunks
    chat_history = ""  # reset chat
    return " Medical PDF loaded successfully!", ""

def chat_with_pdf(user_input):
    global chat_history
    if not index:
        return chat_history + "\n Please upload a PDF first."

    q_emb = embedder.encode(user_input)
    D, I = index.search(np.array([q_emb]), k=3)
    top_chunks = [original_chunks[i] for i in I[0]]
    context = " ".join(top_chunks)
    result = qa_pipeline(question=user_input, context=context)
    answer = result['answer']

    # üë§ = regular user
    chat_history += f"\nüë§ You: {user_input}\nü§ñ Assistant: {answer}\n\n"
    return chat_history

# Build UI
with gr.Blocks(css=".gr-button {background-color: #2563eb !important; color: white} .gr-markdown {font-size: 16px;}") as demo:
    gr.Markdown("""
    <div style="text-align:center; padding: 20px;">
        <h1 style="color:#1e3a8a;"> Smart Medical Assistant</h1>
        <p style="font-size:18px;">Upload a medical PDF and start chatting with it.</p>
    </div>
    """)

    with gr.Row():
        pdf_input = gr.File(type="filepath", label="üìÑ Upload Medical PDF", file_types=[".pdf"])
        status = gr.Textbox(label="üü¢ Status", interactive=False)

    chatbox = gr.Textbox(label="üßæ Chat", lines=20, interactive=False, placeholder="Your conversation will appear here...")
    user_input = gr.Textbox(label="‚úèÔ∏è Type your question and press Enter", placeholder="e.g. What is the diagnosis?", lines=1)

    pdf_input.change(fn=load_pdf, inputs=pdf_input, outputs=[status, chatbox])
    user_input.submit(fn=chat_with_pdf, inputs=user_input, outputs=chatbox)

demo.launch(share=True)